{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The following code is to create environment folder which will contain the kaggle credentials ( You need to add you're credentials )"
      ],
      "metadata": {
        "id": "n-kuDV9KhkpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    f.write('{\"username\":\"Kaggle username\",\"key\":\"Kaggle API Key\"}')\n"
      ],
      "metadata": {
        "id": "SNgCuvQpdjlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will download the dataset from kaggle"
      ],
      "metadata": {
        "id": "JOiVly8FhrRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d jonasbecker98/286k-topic-clustered-news-articles"
      ],
      "metadata": {
        "id": "A-mGtpnqZ0lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzipping the dataset"
      ],
      "metadata": {
        "id": "NZdErwY8hwte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 286k-topic-clustered-news-articles.zip"
      ],
      "metadata": {
        "id": "ycofbHoOel80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing spark"
      ],
      "metadata": {
        "id": "hoAFiZ_Vh2OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "ijJDHqGHfLCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing java, because it will be used by Spark"
      ],
      "metadata": {
        "id": "KlVZAFWph4ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "metadata": {
        "id": "TIarVdWkgIVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dataset is already clustered we have removed all columns added as a result of the K-Means clustring, so we can re-cluster it again."
      ],
      "metadata": {
        "id": "6E5NfkR6h7Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "import numpy as np\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "json_array = []\n",
        "\n",
        "def data_preprocessing():\n",
        "\n",
        "    # First of all we will list all the directories in the dataset directory\n",
        "    dataset_dir = \"/content/clustered_json\"\n",
        "\n",
        "    # array of main directories\n",
        "    dirs_list = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
        "\n",
        "    # array of subdirectories\n",
        "    sub_dirs_list = []\n",
        "    for main_dir in dirs_list:\n",
        "        for d in os.listdir(dataset_dir + \"/\" + main_dir):\n",
        "            if os.path.isdir(os.path.join(dataset_dir, main_dir, d)):\n",
        "                sub_dirs_list.append(dataset_dir + \"/\" + main_dir + \"/\" + d)\n",
        "\n",
        "    # reading the json files one by one then adding tem all in one json array\n",
        "    for sub_dir in sub_dirs_list:\n",
        "        for file in os.listdir(sub_dir):\n",
        "            if file.endswith(\"json\"):\n",
        "                with open(sub_dir + '/' + file, 'r') as f:\n",
        "                    json_data = json.load(f)\n",
        "                    for j in json_data['data']:\n",
        "                        json_array.append(j)\n",
        "\n",
        "\n",
        "    spark = SparkSession.builder.appName('MyApp') \\\n",
        "        .config('spark.ui.port', '4050') \\\n",
        "        .config(\"spark.driver.memory\", \"12g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    df = spark.read.json(spark.sparkContext.parallelize(json_array))\n",
        "\n",
        "    keep_cols = keep_cols = ['maintext', 'date_download', 'date_modify', 'date_publish', 'description', 'language', 'year_month']\n",
        "\n",
        "    new_df = df.select([col(c) for c in keep_cols])\n",
        "    new_df = new_df.withColumn('maintext', new_df['maintext'].cast(StringType()))\n",
        "    new_df = new_df.filter(col(\"maintext\").isNotNull())\n",
        "\n",
        "    # Tokenize the text data\n",
        "    tokenizer = Tokenizer(inputCol=\"maintext\", outputCol=\"words\")\n",
        "    new_df = tokenizer.transform(new_df)\n",
        "\n",
        "    # Remove stop words\n",
        "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "    new_df = remover.transform(new_df)\n",
        "\n",
        "    # Filter out rows with null values in the filtered_words column\n",
        "    new_df = new_df.filter(new_df.filtered_words.isNotNull())\n",
        "\n",
        "    # Compute term frequencies\n",
        "    cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")    \n",
        "    cv_model = cv.fit(new_df)\n",
        "    new_df = cv_model.transform(new_df)\n",
        "\n",
        "    # Compute IDF values\n",
        "    idf = IDF(inputCol=\"raw_features\", outputCol=\"input\")\n",
        "    idf_model = idf.fit(new_df)\n",
        "    new_df = idf_model.transform(new_df)\n",
        "\n",
        "    # create a vector assembler to convert the text column into a vector\n",
        "    vec_assembler = VectorAssembler(inputCols=['input'], outputCol='features')\n",
        "\n",
        "    # apply the vector assembler to your dataframe\n",
        "    new_df = vec_assembler.transform(new_df)\n",
        "\n",
        "    return new_df"
      ],
      "metadata": {
        "id": "U983wsiDfDPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def K_Means(dataframe, k=5, seed=1):\n",
        "\n",
        "    # train the K-means model on the features column\n",
        "    kmeans = KMeans().setK(k).setSeed(seed)\n",
        "    model = kmeans.fit(dataframe.select('features'))\n",
        "\n",
        "    # add the predicted clusters to your original dataframe\n",
        "    df_pred = model.transform(dataframe).withColumnRenamed('prediction', 'kmeans_cluster_id')\n",
        "\n",
        "    return df_pred\n"
      ],
      "metadata": {
        "id": "HAKo5-NV2GU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_preprocessing()"
      ],
      "metadata": {
        "id": "cGQ2riGX2IBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have two methods (Elbow & Silhouette) for finding the optimal K, but in fact usig one of them is enough, and you can choose based on your dataset"
      ],
      "metadata": {
        "id": "xFE3maTxILov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def elbow_method(dataframe, k_range):\n",
        "    # Repartition to increase parallelism and avoid memory issues\n",
        "    dataframe = dataframe.repartition(10)\n",
        "\n",
        "    costs = []\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans().setK(k).setSeed(1)\n",
        "        model = kmeans.fit(dataframe.select('features'))\n",
        "        cost = model.summary.trainingCost\n",
        "        costs.append(cost)\n",
        "\n",
        "    # determine the optimal k\n",
        "    delta = np.diff(costs)\n",
        "    acceleration = np.diff(delta)\n",
        "    optimal_k = k_range[acceleration.argmax() + 1]\n",
        "    return optimal_k\n",
        "\n",
        "elbow_optimal_k = elbow_method(df, range(2, 10))\n",
        "print(f\"Elbow Optimal: {elbow_optimal_k}\")"
      ],
      "metadata": {
        "id": "d86r4v4dUFq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def silhouette_method(dataframe, k_range):\n",
        "\n",
        "    # Repartition to increase parallelism and avoid memory issues\n",
        "    dataframe = dataframe.repartition(10)\n",
        "    \n",
        "    scores = []\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans().setK(k).setSeed(1)\n",
        "        model = kmeans.fit(dataframe.select('features'))\n",
        "        predictions = model.transform(dataframe)\n",
        "        evaluator = ClusteringEvaluator()\n",
        "        score = evaluator.evaluate(predictions)\n",
        "        scores.append(score)\n",
        "   \n",
        "    # determine the optimal k\n",
        "    optimal_k = k_range[scores.index(max(scores))]\n",
        "    \n",
        "    return optimal_k\n",
        "\n",
        "\n",
        "silhouette_optimal_k = silhouette_method(df, range(2,10))\n",
        "print(f\"Silhouette Optimal: {silhouette_optimal_k}\")"
      ],
      "metadata": {
        "id": "aVSqU3XHUHUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = K_Means(df, k=silhouette_optimal_k)\n",
        "print(pred.show(5))"
      ],
      "metadata": {
        "id": "TJgRSAJyIr-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values = pred.select('kmeans_cluster_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "print(f\"Clusters Array: {unique_values}\")"
      ],
      "metadata": {
        "id": "YaA19wu8F-7d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}