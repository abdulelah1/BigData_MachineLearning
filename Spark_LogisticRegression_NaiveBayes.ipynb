{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The following code is to create environment folder which will contain the kaggle credentials  "
      ],
      "metadata": {
        "id": "VqFn1fvzft0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vFZTgbyaLPfW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    f.write('{\"username\":\"aalharabi\",\"key\":\"cc9637f8cc673ab18278a1c9d7c15834\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will download the dataset from kaggle"
      ],
      "metadata": {
        "id": "J4kBKXtogD-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kartik2112/fraud-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKbqwWeFgCxw",
        "outputId": "680eee96-f7da-40c9-96ac-0737d08469cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading fraud-detection.zip to /content\n",
            " 97% 195M/202M [00:00<00:00, 208MB/s]\n",
            "100% 202M/202M [00:01<00:00, 209MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzipping the dataset"
      ],
      "metadata": {
        "id": "cQTdYUhLgIcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip fraud-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3fQfvTNgNfy",
        "outputId": "aec4a0cc-2ced-4f25-fe26-2b70066f6d99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fraud-detection.zip\n",
            "  inflating: fraudTest.csv           \n",
            "  inflating: fraudTrain.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Java"
      ],
      "metadata": {
        "id": "zOObgL24xgWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL8XJms2xisZ",
        "outputId": "62319f0a-3721-4638-9f36-dd37df099af4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.18\" 2023-01-17\n",
            "OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing spark"
      ],
      "metadata": {
        "id": "NzsZJOx5xz_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmm15U1cx1c-",
        "outputId": "f3eae72e-57ce-4c94-9aeb-eedad57af899"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=8eb0efb4a11a4e774a0d7324451d8285f9c1cbe77f76f4dfb8d336942650a6bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data \n",
        "\n",
        "Since the dataset is already splited into train and test, we will combine them in one dataset then we will split them into 80% train, and 20% test"
      ],
      "metadata": {
        "id": "xaRV_BxRgVxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
        "\n",
        "def preprocessing():\n",
        "  spark = SparkSession.builder.appName('MyApp') \\\n",
        "          .config('spark.ui.port', '4050') \\\n",
        "          .config(\"spark.driver.memory\", \"12g\") \\\n",
        "          .getOrCreate()\n",
        "\n",
        "  spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "  train_csv = spark.read.csv('fraudTrain.csv', header=True)\n",
        "  test_csv = spark.read.csv('fraudTest.csv', header=True)\n",
        "\n",
        "  df = train_csv.union(test_csv)\n",
        "  df = df.withColumn('amt', col('amt').cast('double'))\n",
        "  df = df.withColumn('lat', col('lat').cast('double'))\n",
        "  df = df.withColumn('long', col('long').cast('double'))\n",
        "  df = df.withColumn('city_pop', col('city_pop').cast('double'))\n",
        "\n",
        "  # Create a StringIndexer object\n",
        "  category_indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
        "  gender_indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\")\n",
        "\n",
        "  # Fit the StringIndexer to the data\n",
        "  df = category_indexer.fit(df).transform(df)\n",
        "  df = gender_indexer.fit(df).transform(df)\n",
        "\n",
        "  # Use VectorAssembler to create the \"features\" column\n",
        "  input_cols = ['amt', 'lat', 'long', 'city_pop', 'category_index', 'gender_index']\n",
        "  assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "  df = assembler.transform(df)\n",
        "\n",
        "  # Assuming \"data\" is a Spark dataframe with a column named \"features\"\n",
        "  scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "  scalerModel = scaler.fit(df)\n",
        "  df = scalerModel.transform(df)\n",
        "\n",
        "  # Split the data into 80% train and 20% test\n",
        "  train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "  # Drop rows which have NaN and null data\n",
        "  train_df = train_df.dropna()\n",
        "  test_df = test_df.dropna()\n",
        "\n",
        "  # Convert the 'is_fraud' column to integer\n",
        "  train_df = train_df.withColumn('label', col('is_fraud').cast('int'))\n",
        "  test_df = test_df.withColumn('label', col('is_fraud').cast('int'))\n",
        "\n",
        "  # Selecting only the relevant columns for training and testing\n",
        "  selected_cols = ['label','features', 'scaledFeatures']\n",
        "  train_df = train_df.select(selected_cols)\n",
        "  test_df = test_df.select(selected_cols)\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "train_df, test_df = preprocessing()"
      ],
      "metadata": {
        "id": "VoKdgf9Lg8_i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will build the Logistic Regression function then we will feed it with the train and test datasets"
      ],
      "metadata": {
        "id": "brx4U0MXwyKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(train_data, test_data):\n",
        "  # Create a Logistic Regression model\n",
        "  lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
        "\n",
        "  # Train the model on the training data\n",
        "  lr_model = lr.fit(train_data)\n",
        "\n",
        "  # Make predictions on the test data\n",
        "  predictions = lr_model.transform(test_data)\n",
        "\n",
        "  # Evaluate the model\n",
        "  evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
        "  auc = evaluator.evaluate(predictions)\n",
        "\n",
        "  return lr_model, predictions, auc\n",
        "\n",
        "lr_model, lr_predictions, lr_auc = logistic_regression(train_df, test_df)\n",
        "\n",
        "print(\"Area Under ROC Curve (AUC) on test data using Logistic Regression = %g\" % lr_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxonOCq-w50L",
        "outputId": "ef2b1d6f-6da7-4908-b937-5e6f0d72d1cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC Curve (AUC) on test data using Logistic Regression = 0.852991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will build the Naive Bayes function then we will feed it with the train and test datasets"
      ],
      "metadata": {
        "id": "Sbefx5sYTz8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes(train_data, test_data):\n",
        "    # Train a Naive Bayes model\n",
        "    nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol='scaledFeatures')\n",
        "    nb_model = nb.fit(train_data)\n",
        "\n",
        "    # Make predictions on test data\n",
        "    predictions = nb_model.transform(test_data)\n",
        "\n",
        "    # Evaluate the model using accuracy\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "\n",
        "    return nb_model, predictions, auc\n",
        "\n",
        "nb_model, nb_predictions, nb_auc = naive_bayes(train_df, test_df)\n",
        "\n",
        "print(\"Area Under ROC Curve (AUC) on test data using Naive Bayes = %g\" % nb_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V7ktWRnTo5M",
        "outputId": "7c1722b1-2355-467f-a1b7-99da8e991405"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC Curve (AUC) on test data using Naive Bayes = 0.487739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that Logistic Regression Fits our data more than the Naive Bayes"
      ],
      "metadata": {
        "id": "RfCB2Og8T3q1"
      }
    }
  ]
}